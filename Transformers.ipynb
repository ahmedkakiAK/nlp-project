{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88a47e81",
   "metadata": {},
   "source": [
    "# Premier entraînement sur les données Kaggle (binaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0168127",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369f87d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 20000  # Taille du vocabulaire max\n",
    "MAX_LEN = 256           # Longueur max des séquences\n",
    "EMBED_DIM = 100         # Dimension des embeddings\n",
    "HIDDEN_DIM = 128        # Taille des couches LSTM\n",
    "BATCH_SIZE = 16         # Taille des batchs\n",
    "EPOCHS = 10              # Nombre d'époques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e4b6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_df_kaggle = pd.read_csv(\"data/kaggle/preprocessed/train.csv\")\n",
    "test_df_kaggle = pd.read_csv(\"data/kaggle/preprocessed/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231d399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "def encode(vocab, text):\n",
    "    return [vocab.get(tok, 1) for tok in tokenize(text)[:MAX_LEN]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e761020b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "counter_kaggle = Counter()\n",
    "for text in train_df_kaggle[\"text\"]:\n",
    "    tokens_kaggle = tokenize(text)\n",
    "    counter_kaggle.update(tokens)\n",
    "\n",
    "most_common_kaggle = counter_kaggle.most_common(MAX_VOCAB_SIZE - 2)\n",
    "vocab_kaggle = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "for i, (word, _) in enumerate(most_common_kaggle, start=2):\n",
    "    vocab_kaggle[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb1b58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df, vocab, max_len=512):\n",
    "        self.texts = [torch.tensor(encode(vocab, text), dtype=torch.long) for text in df[\"text\"]]\n",
    "        self.labels = torch.tensor(df[\"label\"].values, dtype=torch.long)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Create attention mask\n",
    "        attention_mask = torch.ones_like(text)  # 1 for real tokens, 0 for padding\n",
    "        \n",
    "        # Ensure the text is of max_len\n",
    "        if len(text) > self.max_len:\n",
    "            text = text[:self.max_len]\n",
    "            attention_mask = attention_mask[:self.max_len]\n",
    "        else:\n",
    "            # Padding\n",
    "            padding_length = self.max_len - len(text)\n",
    "            text = torch.cat([text, torch.zeros(padding_length, dtype=torch.long)], dim=0)\n",
    "            attention_mask = torch.cat([attention_mask, torch.zeros(padding_length, dtype=torch.long)], dim=0)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": text,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cfbbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Sort batch by length for packing\n",
    "    texts = [item[\"input_ids\"] for item in batch]\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    attention_masks = [item[\"attention_mask\"] for item in batch]\n",
    "    \n",
    "    # Pad sequences\n",
    "    texts = pad_sequence(texts, batch_first=True, padding_value=0)\n",
    "    attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Convert labels to tensor\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    # Return a dictionary\n",
    "    return {\n",
    "        \"input_ids\": texts,\n",
    "        \"attention_mask\": attention_masks,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e3eabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders\n",
    "train_ds_kaggle = TextDataset(train_df_kaggle, vocab)\n",
    "test_ds_kaggle = TextDataset(test_df_kaggle, vocab)\n",
    "\n",
    "train_loader_kaggle = DataLoader(train_ds_kaggle, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader_kaggle = DataLoader(test_ds_kaggle, batch_size=32, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dad6ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, hidden_dim, num_layers, output_dim, max_len=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.positional_encoding = self._generate_positional_encoding(max_len, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.fc = nn.Linear(embed_dim, output_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        x = self.embedding(input_ids) + self.positional_encoding[:input_ids.size(1)].unsqueeze(0).to(input_ids.device)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # Transformer expects 0 for attend, -inf for mask; convert accordingly\n",
    "            mask = (attention_mask == 0).to(torch.bool)\n",
    "        else:\n",
    "            mask = None\n",
    "\n",
    "        x = self.transformer_encoder(x, src_key_padding_mask=mask)\n",
    "        cls_output = x[:, 0, :]  # Use first token ([CLS]-like)\n",
    "        return self.fc(cls_output)\n",
    "\n",
    "    def _generate_positional_encoding(self, max_len, d_model):\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759bcfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "# Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080405ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_kaggle = TransformerClassifier(\n",
    "    vocab_size=MAX_VOCAB_SIZE,\n",
    "    embed_dim=EMBED_DIM, \n",
    "    hidden_dim=HIDDEN_DIM, \n",
    "    num_heads=2,\n",
    "    num_layers=2,\n",
    "    output_dim=len(train_df_kaggle[\"label\"].unique())\n",
    ")\n",
    "model_kaggle = model_kaggle.to(device)\n",
    "\n",
    "# Optimizer / Loss\n",
    "optimizer_kaggle = torch.optim.AdamW(model_kaggle.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training\n",
    "for epoch in range(20):\n",
    "    model_kaggle.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader_kaggle:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer_kaggle.zero_grad()\n",
    "        outputs = model_kaggle(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_kaggle.step()\n",
    "        total_loss += loss.item()\n",
    "        mean_loss = total_loss/len(train_loader_kaggle)\n",
    "    print(f\"Epoch {epoch+1} Loss_Total: {total_loss:.4f}; Mean_Loss: {mean_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c50b9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kaggle.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader_kaggle:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels']\n",
    "        outputs = model_kaggle(input_ids, attention_mask)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "print(classification_report(all_labels, all_preds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b690de8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6612d30a",
   "metadata": {},
   "source": [
    "# Deuxième entraînement sur les données ISOT (binaire)\n",
    "C'est exactement la même chose. On donne un autre nom au modèle pour pouvoir les conserver et les évaluer plus tard pour la généralisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fec8cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_df_isot = pd.read_csv(\"data/isot/preprocessed/train.csv\")\n",
    "test_df_isot = pd.read_csv(\"data/isot/preprocessed/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff99a27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "counter_isot = Counter()\n",
    "for text in train_df_isot[\"text\"]:\n",
    "    tokens = tokenize(text)\n",
    "    counter_isot.update(tokens)\n",
    "\n",
    "most_common_isot = counter_isot.most_common(MAX_VOCAB_SIZE - 2)\n",
    "vocab_isot = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "for i, (word, _) in enumerate(most_common_isot, start=2):\n",
    "    vocab_isot[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0994e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders\n",
    "train_ds_isot = TextDataset(train_df_isot, vocab)\n",
    "test_ds_isot = TextDataset(test_df_isot, vocab)\n",
    "\n",
    "train_loader_isot = DataLoader(train_ds_isot, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader_isot = DataLoader(test_ds_isot, batch_size=32, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb05acfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model_isot = TransformerClassifier(\n",
    "    vocab_size=MAX_VOCAB_SIZE,\n",
    "    embed_dim=EMBED_DIM, \n",
    "    hidden_dim=HIDDEN_DIM, \n",
    "    num_heads=2,\n",
    "    num_layers=2,\n",
    "    output_dim=len(train_df_isot[\"label\"].unique())\n",
    ")\n",
    "model_isot = model_isot.to(device)\n",
    "\n",
    "# Optimizer / Loss\n",
    "optimizer_isot = torch.optim.AdamW(model_isot.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training\n",
    "for epoch in range(20):\n",
    "    model_isot.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader_isot:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer_isot.zero_grad()\n",
    "        outputs = model_isot(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_isot.step()\n",
    "        total_loss += loss.item()\n",
    "        mean_loss = total_loss/len(train_loader_isot)\n",
    "    print(f\"Epoch {epoch+1} Loss_Total: {total_loss:.4f}; Mean_Loss: {mean_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a75107",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_isot.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader_isot:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels']\n",
    "        outputs = model_isot(input_ids, attention_mask)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "print(classification_report(all_labels, all_preds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f7a68d",
   "metadata": {},
   "source": [
    "# Généralisation : Entrainement sur Kaggle et évaluation sur ISOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b25909",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_kaggle.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader_isot:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels']\n",
    "        outputs = model_kaggle(input_ids, attention_mask)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "print(classification_report(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8d551a",
   "metadata": {},
   "source": [
    "# Généralisation : Entrainement sur ISOT et évaluation sur Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6fba1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_isot.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader_kaggle:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels']\n",
    "        outputs = model_isot(input_ids, attention_mask)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "print(classification_report(all_labels, all_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
