{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e94ed73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "isot_train = pd.read_csv('data/isot/preprocessed/train.csv')\n",
    "isot_test = pd.read_csv('data/isot/preprocessed/test.csv')\n",
    "\n",
    "\n",
    "kaggle_train = pd.read_csv('data/kaggle/preprocessed/train.csv')\n",
    "kaggle_test = pd.read_csv('data/kaggle/preprocessed/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66399d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "# Fonction pour préparer les données\n",
    "def prepare_data(train_df, test_df):\n",
    "    # Conversion des DataFrames pandas en objets datasets\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "    # Charger le tokenizer BERT\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Fonction de tokenisation\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "    # Appliquer la tokenisation aux datasets\n",
    "    train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "    test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    return train_dataset, test_dataset, tokenizer\n",
    "\n",
    "# Fonction pour créer et entraîner le modèle\n",
    "def train_model(train_df, test_df):\n",
    "    # Préparer les données\n",
    "    train_dataset, test_dataset, tokenizer = prepare_data(train_df, test_df)\n",
    "\n",
    "    # Charger le modèle pré-entraîné BERT pour la classification\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "    # Définir les arguments d'entraînement\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',          # Dossier de sortie\n",
    "        num_train_epochs=3,              # Nombre d'époques\n",
    "        per_device_train_batch_size=8,   # Taille du batch pour l'entraînement\n",
    "        per_device_eval_batch_size=16,   # Taille du batch pour l'évaluation\n",
    "        evaluation_strategy=\"epoch\",     # Évaluation après chaque époque\n",
    "        logging_dir='./logs',            # Dossier pour les logs\n",
    "    )\n",
    "\n",
    "    # Créer le trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,                         # Le modèle\n",
    "        args=training_args,                  # Les arguments d'entraînement\n",
    "        train_dataset=train_dataset,         # Dataset d'entraînement\n",
    "        eval_dataset=test_dataset,           # Dataset de test\n",
    "    )\n",
    "\n",
    "    # Entraîner le modèle\n",
    "    trainer.train()\n",
    "\n",
    "    # Sauvegarder le modèle\n",
    "    model.save_pretrained('./fake_news_model')\n",
    "    tokenizer.save_pretrained('./fake_news_model')\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "# Fonction pour évaluer le modèle\n",
    "def evaluate_model(model, tokenizer, test_df):\n",
    "    # Tokenisation du test set\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "    test_dataset = test_dataset.map(lambda x: tokenizer(x['text'], padding=\"max_length\", truncation=True, max_length=512), batched=True)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    results = trainer.evaluate(test_dataset)\n",
    "    print(f\"Évaluation sur le dataset de test: {results}\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71334322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 5068/5068 [00:27<00:00, 187.67 examples/s]\n",
      "Map: 100%|██████████| 1267/1267 [00:07<00:00, 176.30 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Accelerator.__init__() got an unexpected keyword argument 'dispatch_batches'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Exemple d'utilisation\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Exemple de DataFrame train et test (remplace par tes propres données)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Entraînement du modèle\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkaggle_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkaggle_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Évaluation sur le dataset de test\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     evaluate_model(model, tokenizer, kaggle_test)\n",
      "Cell \u001b[0;32mIn[4], line 43\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(train_df, test_df)\u001b[0m\n\u001b[1;32m     33\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     34\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m'\u001b[39m,          \u001b[38;5;66;03m# Dossier de sortie\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,              \u001b[38;5;66;03m# Nombre d'époques\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m     logging_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./logs\u001b[39m\u001b[38;5;124m'\u001b[39m,            \u001b[38;5;66;03m# Dossier pour les logs\u001b[39;00m\n\u001b[1;32m     40\u001b[0m )\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Créer le trainer\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                         \u001b[49m\u001b[38;5;66;43;03m# Le modèle\u001b[39;49;00m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                  \u001b[49m\u001b[38;5;66;43;03m# Les arguments d'entraînement\u001b[39;49;00m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# Dataset d'entraînement\u001b[39;49;00m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# Dataset de test\u001b[39;49;00m\n\u001b[1;32m     48\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Entraîner le modèle\u001b[39;00m\n\u001b[1;32m     51\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/trainer.py:343\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_in_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 343\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_accelerator_and_postprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# memory metrics - must set up as early as possible\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_tracker \u001b[38;5;241m=\u001b[39m TrainerMemoryTracker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mskip_memory_metrics)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/trainer.py:3882\u001b[0m, in \u001b[0;36mTrainer.create_accelerator_and_postprocess\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3879\u001b[0m gradient_accumulation_plugin \u001b[38;5;241m=\u001b[39m GradientAccumulationPlugin(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgrad_acc_kwargs)\n\u001b[1;32m   3881\u001b[0m \u001b[38;5;66;03m# create accelerator object\u001b[39;00m\n\u001b[0;32m-> 3882\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator \u001b[38;5;241m=\u001b[39m \u001b[43mAccelerator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdispatch_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3884\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_batches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeepspeed_plugin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepspeed_plugin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_plugin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_accumulation_plugin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3887\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3888\u001b[0m \u001b[38;5;66;03m# some Trainer classes need to use `gather` instead of `gather_for_metrics`, thus we store a flag\u001b[39;00m\n\u001b[1;32m   3889\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather_for_metrics\n",
      "\u001b[0;31mTypeError\u001b[0m: Accelerator.__init__() got an unexpected keyword argument 'dispatch_batches'"
     ]
    }
   ],
   "source": [
    "# Exemple d'utilisation\n",
    "if __name__ == \"__main__\":\n",
    "    # Exemple de DataFrame train et test (remplace par tes propres données)\n",
    "\n",
    "\n",
    "    # Entraînement du modèle\n",
    "    model, tokenizer = train_model(kaggle_train, kaggle_test)\n",
    "\n",
    "    # Évaluation sur le dataset de test\n",
    "    evaluate_model(model, tokenizer, kaggle_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
