{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6587e05",
   "metadata": {},
   "source": [
    "# Deep Learning Model Experiments: RNN vs. LSTM vs. GRU\n",
    "\n",
    "In this notebook, we evaluate the performance of Recurrent Neural Networks (RNNs) and two of their variants: Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU). These architectures use memory cells to retain or discard relevant information, effectively addressing the vanishing gradient problem. \n",
    "\n",
    "The task at hand is fake news detection, which is a binary classification problem. To tackle this, we use two Kaggle datasets (ISOT and Kaggle True or Fake) containing labeled news articles categorized as either True or Fake. The datasets have already been preprocessed: all text has been lowercased, special characters have been removed, and the data has been split into training and test sets, making it ready for vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7954674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from collections import Counter\n",
    "import copy\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbf1cf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed Hyperparameters\n",
    "MAX_VOCAB_SIZE = 30000\n",
    "MAX_LEN = 512\n",
    "EMBED_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4023bc5",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f439a31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets: choose ISOT or Kaggle True or Fake datasets\n",
    "\n",
    "train_df = pd.read_csv(\"data/kaggle/preprocessed/train.csv\")\n",
    "test_df = pd.read_csv(\"data/kaggle/preprocessed/test.csv\")\n",
    "\n",
    "# train_df = pd.read_csv(\"data/isot/preprocessed/train.csv\")\n",
    "# test_df = pd.read_csv(\"data/isot/preprocessed/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3a7767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "def encode(vocab, text):\n",
    "    return [vocab.get(tok, 1) for tok in tokenize(text)[:MAX_LEN]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2eb02aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "counter = Counter()\n",
    "for text in train_df[\"text\"]:\n",
    "    tokens = tokenize(text)\n",
    "    counter.update(tokens)\n",
    "\n",
    "most_common = counter.most_common(MAX_VOCAB_SIZE - 2)\n",
    "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "for i, (word, _) in enumerate(most_common, start=2):\n",
    "    vocab[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f81145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.texts = [torch.tensor(encode(vocab, text), dtype=torch.long) for text in df[\"text\"]]\n",
    "        self.labels = torch.tensor(df[\"label\"].values, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb518d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    texts = pad_sequence(texts, batch_first=True, padding_value=0)\n",
    "    texts = texts[:, :MAX_LEN]  # truncate if needed\n",
    "    return texts, torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa3812af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Validation split\n",
    "val_ratio = 0.2\n",
    "total_len = len(train_df)\n",
    "val_len = int(total_len * val_ratio)\n",
    "train_len = total_len - val_len\n",
    "\n",
    "# Full train dataset\n",
    "full_train_ds = TextDataset(train_df)\n",
    "\n",
    "# Split train/val\n",
    "train_ds, val_ds = random_split(full_train_ds, [train_len, val_len], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Test DataLoader\n",
    "test_ds = TextDataset(test_df)\n",
    "test_loader = DataLoader(test_ds, batch_size=32, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c27ff0b",
   "metadata": {},
   "source": [
    "## Defining the models\n",
    "We implement our three models into classes. Our models would have the same hyperparameters: embedding and hidden dimensions, dropout rate, number of layers in order to be able to compare their performances efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd18ee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Model\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=vocab[\"<PAD>\"])\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, hidden = self.rnn(x)\n",
    "        hidden = self.dropout(hidden[-1])\n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c09a1ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=vocab[\"<PAD>\"])\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        return self.fc(hidden[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fcf61592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU Model\n",
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=vocab[\"<PAD>\"])\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, hidden = self.gru(x)\n",
    "        hidden = self.dropout(hidden[-1])\n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72c35dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the accuracy for the evaluation\n",
    "def accuracy_fn(y_pred, y_true):\n",
    "    preds = torch.argmax(y_pred, dim=1)\n",
    "    correct = (preds == y_true).sum().item()\n",
    "    total = y_true.size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ef6cf4",
   "metadata": {},
   "source": [
    "## RNN Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab306925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model\n",
    "model = RNNClassifier(len(vocab), embedding_dim=100, hidden_dim=128, output_dim=len(train_df[\"label\"].unique()))\n",
    "model = model.to(device)\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-1)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4f1b521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training Loss: 2.0025\n",
      "Epoch 1 Validation Loss: 0.9755\n",
      "Epoch 2 Training Loss: 1.6803\n",
      "Epoch 2 Validation Loss: 1.0395\n",
      "Epoch 3 Training Loss: 2.1502\n",
      "Epoch 3 Validation Loss: 0.9600\n",
      "Epoch 4 Training Loss: 1.8748\n",
      "Epoch 4 Validation Loss: 1.3098\n",
      "Epoch 5 Training Loss: 1.8470\n",
      "Epoch 5 Validation Loss: 2.0260\n",
      "Epoch 6 Training Loss: 1.7276\n",
      "Epoch 6 Validation Loss: 1.8981\n",
      "Epoch 7 Training Loss: 1.7383\n",
      "Epoch 7 Validation Loss: 0.8439\n",
      "Epoch 8 Training Loss: 1.9073\n",
      "Epoch 8 Validation Loss: 2.0704\n",
      "Epoch 9 Training Loss: 2.1005\n",
      "Epoch 9 Validation Loss: 0.9550\n",
      "Epoch 10 Training Loss: 1.6733\n",
      "Epoch 10 Validation Loss: 1.2588\n",
      "Epoch 11 Training Loss: 1.6504\n",
      "Epoch 11 Validation Loss: 1.1074\n",
      "Epoch 12 Training Loss: 1.8242\n",
      "Epoch 12 Validation Loss: 1.0688\n",
      "Epoch 13 Training Loss: 1.8849\n",
      "Epoch 13 Validation Loss: 2.4650\n",
      "Epoch 14 Training Loss: 1.9479\n",
      "Epoch 14 Validation Loss: 1.1213\n",
      "Epoch 15 Training Loss: 1.8099\n",
      "Epoch 15 Validation Loss: 1.0166\n",
      "Epoch 16 Training Loss: 2.0481\n",
      "Epoch 16 Validation Loss: 1.2261\n",
      "Epoch 17 Training Loss: 1.7987\n",
      "Epoch 17 Validation Loss: 0.9327\n",
      "Epoch 18 Training Loss: 1.6417\n",
      "Epoch 18 Validation Loss: 1.1576\n",
      "Epoch 19 Training Loss: 1.7354\n",
      "Epoch 19 Validation Loss: 1.5125\n",
      "Epoch 20 Training Loss: 1.8990\n",
      "Epoch 20 Validation Loss: 1.8414\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} Training Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in val_loader:\n",
    "            x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "            outputs = model(x_val)\n",
    "            loss = criterion(outputs, y_val)\n",
    "            val_loss += loss.item()\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1} Validation Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f61148a",
   "metadata": {},
   "source": [
    "We observe here, that our RNN model is not learning. When computing the gradient, RNNs can suffer from the severe attenuation of the gradient because of the multiple time steps (here the different tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1ea23443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5361\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.57      0.55       624\n",
      "           1       0.55      0.51      0.53       643\n",
      "\n",
      "    accuracy                           0.54      1267\n",
      "   macro avg       0.54      0.54      0.54      1267\n",
      "weighted avg       0.54      0.54      0.54      1267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "total_acc = 0\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        outputs = model(x_batch)\n",
    "        predictions = torch.argmax(outputs, dim=1).cpu()\n",
    "        acc = accuracy_fn(outputs, y_batch)\n",
    "        total_acc += acc\n",
    "\n",
    "        y_true.extend(y_batch.cpu().numpy())\n",
    "        y_pred.extend(predictions.numpy())\n",
    "\n",
    "avg_acc = total_acc / len(test_loader)\n",
    "print(f\"Test Accuracy: {avg_acc:.4f}\")\n",
    "\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c0a467",
   "metadata": {},
   "source": [
    "Thus, the RNN model is no better than a random prediction on this dataset. We will rely on the other variants for better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f56e8c3",
   "metadata": {},
   "source": [
    "## LSTM Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b9efea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = LSTMClassifier(len(vocab), embedding_dim=100, hidden_dim=128, output_dim=len(train_df[\"label\"].unique()), dropout=0)\n",
    "model = model.to(device)\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "45b8cb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training Loss: 0.6441\n",
      "Epoch 1 Validation Loss: 0.6237\n",
      "Epoch 2 Training Loss: 0.4845\n",
      "Epoch 2 Validation Loss: 0.6271\n",
      "Epoch 3 Training Loss: 0.3460\n",
      "Epoch 3 Validation Loss: 0.6603\n",
      "Epoch 4 Training Loss: 0.3412\n",
      "Epoch 4 Validation Loss: 0.7411\n",
      "Epoch 5 Training Loss: 0.2805\n",
      "Epoch 5 Validation Loss: 0.7669\n",
      "Epoch 6 Training Loss: 0.3515\n",
      "Epoch 6 Validation Loss: 0.6237\n",
      "Epoch 7 Training Loss: 0.2331\n",
      "Epoch 7 Validation Loss: 0.7308\n",
      "Epoch 8 Training Loss: 0.2194\n",
      "Epoch 8 Validation Loss: 0.7504\n",
      "Epoch 9 Training Loss: 0.2482\n",
      "Epoch 9 Validation Loss: 0.8766\n",
      "Epoch 10 Training Loss: 0.2427\n",
      "Epoch 10 Validation Loss: 0.8295\n",
      "Epoch 11 Training Loss: 0.2016\n",
      "Epoch 11 Validation Loss: 0.8000\n",
      "Epoch 12 Training Loss: 0.2014\n",
      "Epoch 12 Validation Loss: 0.8862\n",
      "Epoch 13 Training Loss: 0.3370\n",
      "Epoch 13 Validation Loss: 0.7104\n",
      "Epoch 14 Training Loss: 0.2533\n",
      "Epoch 14 Validation Loss: 0.7450\n",
      "Epoch 15 Training Loss: 0.2124\n",
      "Epoch 15 Validation Loss: 1.3705\n",
      "Epoch 16 Training Loss: 0.2287\n",
      "Epoch 16 Validation Loss: 0.7595\n",
      "Epoch 17 Training Loss: 0.2115\n",
      "Epoch 17 Validation Loss: 0.6965\n",
      "Epoch 18 Training Loss: 0.2103\n",
      "Epoch 18 Validation Loss: 0.7836\n",
      "Epoch 19 Training Loss: 0.2029\n",
      "Epoch 19 Validation Loss: 0.9317\n",
      "Epoch 20 Training Loss: 0.1990\n",
      "Epoch 20 Validation Loss: 0.9260\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} Training Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in val_loader:\n",
    "            x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "            outputs = model(x_val)\n",
    "            loss = criterion(outputs, y_val)\n",
    "            val_loss += loss.item()\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1} Validation Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "03bf7cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8007\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.73      0.78       624\n",
      "           1       0.77      0.87      0.82       643\n",
      "\n",
      "    accuracy                           0.80      1267\n",
      "   macro avg       0.81      0.80      0.80      1267\n",
      "weighted avg       0.81      0.80      0.80      1267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "total_acc = 0\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        outputs = model(x_batch)\n",
    "        predictions = torch.argmax(outputs, dim=1).cpu()\n",
    "        acc = accuracy_fn(outputs, y_batch)\n",
    "        total_acc += acc\n",
    "\n",
    "        y_true.extend(y_batch.cpu().numpy())\n",
    "        y_pred.extend(predictions.numpy())\n",
    "\n",
    "avg_acc = total_acc / len(test_loader)\n",
    "print(f\"Test Accuracy: {avg_acc:.4f}\")\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fa6d19",
   "metadata": {},
   "source": [
    "## GRU Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d920d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = GRUClassifier(len(vocab), embedding_dim=100, hidden_dim=128, output_dim=len(train_df[\"label\"].unique()))\n",
    "model = model.to(device)\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "10b8a48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Training Loss: 0.6442\n",
      "Epoch 1 Validation Loss: 0.6205\n",
      "Epoch 2 Training Loss: 0.5040\n",
      "Epoch 2 Validation Loss: 0.6142\n",
      "Epoch 3 Training Loss: 0.3452\n",
      "Epoch 3 Validation Loss: 0.7879\n",
      "Epoch 4 Training Loss: 0.2836\n",
      "Epoch 4 Validation Loss: 0.8194\n",
      "Epoch 5 Training Loss: 0.2648\n",
      "Epoch 5 Validation Loss: 0.7548\n",
      "Epoch 6 Training Loss: 0.2614\n",
      "Epoch 6 Validation Loss: 0.9116\n",
      "Epoch 7 Training Loss: 0.2206\n",
      "Epoch 7 Validation Loss: 0.8761\n",
      "Epoch 8 Training Loss: 0.1926\n",
      "Epoch 8 Validation Loss: 0.8360\n",
      "Epoch 9 Training Loss: 0.2249\n",
      "Epoch 9 Validation Loss: 0.8442\n",
      "Epoch 10 Training Loss: 0.1796\n",
      "Epoch 10 Validation Loss: 0.7978\n",
      "Epoch 11 Training Loss: 0.1582\n",
      "Epoch 11 Validation Loss: 0.5567\n",
      "Epoch 12 Training Loss: 0.1116\n",
      "Epoch 12 Validation Loss: 0.5859\n",
      "Epoch 13 Training Loss: 0.0737\n",
      "Epoch 13 Validation Loss: 0.8061\n",
      "Epoch 14 Training Loss: 0.0679\n",
      "Epoch 14 Validation Loss: 0.8899\n",
      "Epoch 15 Training Loss: 0.0572\n",
      "Epoch 15 Validation Loss: 0.9412\n",
      "Epoch 16 Training Loss: 0.0642\n",
      "Epoch 16 Validation Loss: 0.4911\n",
      "Epoch 17 Training Loss: 0.0603\n",
      "Epoch 17 Validation Loss: 0.7248\n",
      "Epoch 18 Training Loss: 0.0447\n",
      "Epoch 18 Validation Loss: 0.7207\n",
      "Epoch 19 Training Loss: 0.0354\n",
      "Epoch 19 Validation Loss: 0.7789\n",
      "Epoch 20 Training Loss: 0.0346\n",
      "Epoch 20 Validation Loss: 0.7401\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} Training Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in val_loader:\n",
    "            x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "            outputs = model(x_val)\n",
    "            loss = criterion(outputs, y_val)\n",
    "            val_loss += loss.item()\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1} Validation Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eea3dd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8409\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.81      0.83       624\n",
      "           1       0.82      0.87      0.85       643\n",
      "\n",
      "    accuracy                           0.84      1267\n",
      "   macro avg       0.84      0.84      0.84      1267\n",
      "weighted avg       0.84      0.84      0.84      1267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "total_acc = 0\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        outputs = model(x_batch)\n",
    "        predictions = torch.argmax(outputs, dim=1).cpu()\n",
    "        acc = accuracy_fn(outputs, y_batch)\n",
    "        total_acc += acc\n",
    "\n",
    "        y_true.extend(y_batch.cpu().numpy())\n",
    "        y_pred.extend(predictions.numpy())\n",
    "\n",
    "avg_acc = total_acc / len(test_loader)\n",
    "print(f\"Test Accuracy: {avg_acc:.4f}\")\n",
    "\n",
    "print(classification_report(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
